{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97290e12-e428-4219-8bb7-faa25cc2d27d",
   "metadata": {},
   "source": [
    "### BERT分類モデル特徴量作成ファイル"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f8e7c8-97c6-4a7d-bdc0-1a80f230cd32",
   "metadata": {},
   "source": [
    "※実行毎にRestart, 変数指定をしてください。<br>\n",
    "\n",
    "【このファイルでの変数指定順序】<br>\n",
    "スパンモデル用特徴量を作成してください。<br>\n",
    "(365, 2-3hour)<br>\n",
    "① span: 365 , col: title , model: whole<br>\n",
    "② span: 365 , col: story , model: whole<br>\n",
    "③ span: 365 , col: keyword , model: whole<br>\n",
    "④ span: 365 , col: title , model: v2<br>\n",
    "⑤ span: 365 , col: story , model: v2<br>\n",
    "⑥ span: 365 , col: keyword , model: v2<br>\n",
    "(730, 2-3hour)<br>\n",
    "⑦ span: 730 , col: title , model: whole<br>\n",
    "⑧ span: 730 , col: story , model: whole<br>\n",
    "⑨ span: 730 , col: keyword , model: whole<br>\n",
    "⑩ span: 730 , col: title , model: v2<br>\n",
    "⑪ span: 730 , col: story , model: v2<br>\n",
    "⑫ span: 730 , col: keyword , model: v2<br>\n",
    "\n",
    "メインモデル用特徴量を作成してください。<br>\n",
    "(0, 3-4hour)\n",
    "⑬ span: 0 , col: title , model: whole<br>\n",
    "⑭ span: 0 , col: story , model: whole<br>\n",
    "⑮ span: 0 , col: keyword , model: whole<br>\n",
    "⑯ span: 0 , col: title , model: v2<br>\n",
    "⑰ span: 0 , col: story , model: v2<br>\n",
    "⑱ span: 0 , col: keyword , model: v2<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d568176c-458b-44de-951b-de4f3fb816d0",
   "metadata": {},
   "source": [
    "### 目次"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7624a6-b96f-461d-b163-16d4a6107519",
   "metadata": {},
   "source": [
    "・ライブラリインストール<br>\n",
    "・ランダム指定<br>\n",
    "・変数指定<br>\n",
    "・定数<br>\n",
    "・データ読み込み・前処理<br>\n",
    "・最大トークン数決定（参考）<br>\n",
    "・関数<br>\n",
    "・BERT訓練〜特徴量作成<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c5b029-909f-4757-897a-0100f891aa64",
   "metadata": {},
   "source": [
    "### ライブラリインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3bcd06a-0195-4950-9215-fc51f7db6bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pickle\n",
    "import re\n",
    "import unicodedata\n",
    "import warnings\n",
    "\n",
    "import GPUtil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex\n",
    "import scipy as sp\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertJapaneseTokenizer, BertForSequenceClassification, AdamW\n",
    "  \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01ad274-38b3-48ff-b2b3-cec2a7104605",
   "metadata": {},
   "source": [
    "### ランダム指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5074334-3bda-4fed-af8b-f3a6d58c3134",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bea162e-0bd3-4b54-8e5b-8322db0a25e2",
   "metadata": {},
   "source": [
    "### 変数指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfdfa5e1-bbee-488f-8e76-4fa162b8ed2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B_mainと同じディレクトリ\n",
    "# BASE_DIR = r'D:/Data/Nishika/Extending_bookmarks/data/'\n",
    "BASE_DIR = r''\n",
    "# MODEL_NAME\n",
    "MODEL_NAME = r'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "# MODEL_NAME = r'cl-tohoku/bert-base-japanese-v2'\n",
    "span = 0 # 全て:0, 365:365日(1年), 730:730日(2年)\n",
    "col = 'keyword' # 'title', 'story', 'keyword'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bac175-adc4-40c9-a22c-9cd9c706e23a",
   "metadata": {},
   "source": [
    "###　定数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ee7a27b-028c-439a-9795-dd128046f19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_PATTERN = re.compile(r'http[\\w:./\\d]+') # URL\n",
    "DATE_PATTERN = re.compile(r'\\d+/\\d+/\\d+') # DATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aa6480-a4b0-4b68-bfb9-5d3843b583a7",
   "metadata": {},
   "source": [
    "### データ読み込み・前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e380c33f-77d1-48fb-94de-f4767838a4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv(BASE_DIR + 'train.csv')\n",
    "raw.keyword.fillna('', inplace=True)\n",
    "raw_test = pd.read_csv(BASE_DIR + 'test.csv')\n",
    "raw_test.keyword.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8c04d90-1786-44ab-93bd-7b76e274d3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def firstup_prep(df):\n",
    "    df = df.copy()\n",
    "    # 経過日数カラム\n",
    "    elapsed_days = lambda x: (base_date - datetime.datetime.strptime(x.split()[0], '%Y-%m-%d')).days\n",
    "    # データをダウンロードした日をベース\n",
    "    base_date = datetime.datetime(2021,9,29)\n",
    "    df['elapsed_days'] = df.general_firstup.apply(elapsed_days)\n",
    "    df.drop('general_firstup', axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "902775f9-2e89-4542-9db9-57ba7d68c352",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = firstup_prep(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f068dae6-9cad-4e11-9115-e1355c62e115",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (span == 365) | (span == 730):\n",
    "    days = df_train.elapsed_days.unique().min() + span\n",
    "    df_train = df_train[df_train.elapsed_days < days]\n",
    "    df_train = df_train.drop('elapsed_days', axis=1)\n",
    "    df_train.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "606a9dae-04d2-4013-b024-b2621adf3743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6615e943-0ac4-4d71-9c33-f94589f5cd84",
   "metadata": {},
   "source": [
    "### 最大token数決定（参考）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30538be8-6b48-4a7a-b556-92d77f630461",
   "metadata": {},
   "source": [
    "最大token数は訓練データの90%分位点で決めました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b242e615-54b7-4fad-9221-629a27ad9e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 【title】\n",
    "# token数を確認 (train)-> mean:10, 中央値:7, 第三四分位点:12, max:100, 90:20\n",
    "# token数を確認(test) -> mean:10, 中央値:7, 第三四分位点:12, max:72, 90:22 \n",
    "# 最大token数: 12(第三四分位点)+2で設定 -> 20(90%分位点)+2に変更\n",
    "\n",
    "# 【story】\n",
    "# token数を確認(train) -> mean:86, 中央値:54, 第三四分位点:115, max:735, 90:202\n",
    "# token数を確認(test) -> mean:63, 中央値:38, 第三四分位点:82, max:999, 90:146\n",
    "# 最大token数: 115(第三四分位点)+2で設定 -> 202(90%分位点)+2に変更\n",
    "\n",
    "# 【keyword】\n",
    "# token数を確認(train) -> mean:14, 中央値:11, 第三四分位点:19, max:117, 90:28\n",
    "# token数を確認(test) -> mean:14, 中央値:9, 第三四分位点:16, max:78, 90:25\n",
    "# 最大token数: 19(第三四分位点)+2で設定 -> 28(90%分位点)+2に変更"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af3bb3a3-7648-44f7-b971-bbe8da51ee6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 最大token数を確認\n",
    "# def show_tokendata(df, col):\n",
    "#     len_list = []\n",
    "#     for i in range(len(df)):\n",
    "#         text = text_normalize(df[col][i])\n",
    "#         text = URL_PATTERN.sub('', text)\n",
    "#         text = DATE_PATTERN.sub('', text)\n",
    "#         text = text.translate(str.maketrans({\"\\n\":\"\", \"\\t\":\"\", \"\\r\":\"\"}))\n",
    "#         len_token = len(tokenizer.tokenize(text))\n",
    "#         len_list.append(len_token)\n",
    "#     len_list = np.array(len_list)    \n",
    "#     print('mean:', np.mean(len_list).round(), 'max:', np.max(len_list))\n",
    "#     print('percentile(50,75,90): ', np.percentile(len_list, [50, 75, 90]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dce7f3b7-c39e-4ef9-aef8-4e0b1d56ed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_tokendata(raw, 'title')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc11a625-1b71-4065-a061-c39700df08fb",
   "metadata": {},
   "source": [
    "### 関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63562fbb-84c0-432f-bb79-042dc191e834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_normalize(text):\n",
    "    # 文字列を正規化\n",
    "    return unicodedata.normalize('NFKC', text)\n",
    "\n",
    "\n",
    "def sharp_del(token_list):\n",
    "    \"\"\"トークンから##を消して連結\"\"\"\n",
    "    count = 0\n",
    "    for t in token_list:\n",
    "        if '##' in t:\n",
    "            count += 1\n",
    "    while count > 0:\n",
    "        for i in range(len(token_list)):\n",
    "            if '##' in token_list[i]:\n",
    "                token_list[i-1] = token_list[i-1] + token_list[i][2:]\n",
    "                del token_list[i]\n",
    "                count -= 1\n",
    "                break\n",
    "    return token_list\n",
    "\n",
    "\n",
    "def bert_prep(df, col):\n",
    "    \"\"\"BERTに入力できるようにデータを前処理\"\"\"\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    c = 0\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        text = text_normalize(df[col][i])\n",
    "        text = URL_PATTERN.sub('', text)\n",
    "        text = DATE_PATTERN.sub('', text)\n",
    "        text = text.translate(str.maketrans({\"\\n\":\"\", \"\\t\":\"\", \"\\r\":\"\"}))\n",
    "        \n",
    "        if col == 'title': # 14 < 22\n",
    "            max_length = 22\n",
    "        if col == 'story': # 117 < 204\n",
    "            max_length = 204\n",
    "        if col == 'keyword':\n",
    "            max_length = 30 # 21 < 30\n",
    "        \n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True, # special token追加\n",
    "            max_length=max_length, # 最大長固定\n",
    "            pad_to_max_length=True, # padding\n",
    "            return_attention_mask=True, # attention mask作成\n",
    "            return_tensors='pt', # tensor返す\n",
    "            truncation=True, # 最大長で切り捨て\n",
    "            )\n",
    "        # tokenIDを取得\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        # attention maskを取得\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "        c += 1\n",
    "        print(f'\\r{c}', end='')\n",
    "    print()\n",
    "    \n",
    "    # データ整形\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    \n",
    "    if 'fav_novel_cnt_bin' in df.columns:\n",
    "        labels = torch.tensor(df.fav_novel_cnt_bin.values)\n",
    "        return input_ids, attention_masks, labels\n",
    "    else:\n",
    "        # テストデータ用\n",
    "        return input_ids, attention_masks\n",
    "\n",
    "\n",
    "def train():\n",
    "    # 訓練\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch[0].to(device)\n",
    "        input_masks = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, \n",
    "                        attention_mask=input_masks, \n",
    "                        labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.cpu().item()\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def one_hot(labels_list):\n",
    "    # 5カラムのワンホット\n",
    "    l = np.zeros((len(labels_list), 5))\n",
    "    for i, label in enumerate(labels_list):\n",
    "        l[i][label] = 1\n",
    "    return l\n",
    "\n",
    "\n",
    "def validation():\n",
    "    # 検証\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    logits_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    for batch_data in validation_dataloader:\n",
    "        labels_list += list(batch_data[2].numpy().reshape(-1))\n",
    "        input_ids = batch_data[0].to(device)\n",
    "        input_masks = batch_data[1].to(device)\n",
    "        labels = batch_data[2].to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, \n",
    "                            attention_mask=input_masks,\n",
    "                            labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        val_loss += loss.cpu().item()\n",
    "        logits_list.append(logits.cpu().numpy())\n",
    "\n",
    "    # logloss, acc\n",
    "    df_logits = pd.DataFrame()\n",
    "    for l in logits_list:\n",
    "        df_logits = df_logits.append(pd.DataFrame(l))\n",
    "    df_logits = sp.special.softmax(df_logits, axis=1)\n",
    "    one_hot_labels_list = one_hot(labels_list)\n",
    "    logloss = log_loss(one_hot_labels_list, df_logits)\n",
    "    acc = accuracy_score(labels_list, df_logits.values.argmax(axis=1))\n",
    "    \n",
    "    return val_loss, logloss, acc, df_logits\n",
    "\n",
    "\n",
    "def make_train_features(skf_logits_list, skf_val_index_list, col):\n",
    "    # 訓練データ用の特徴量\n",
    "    # reshapeできいないため、データフレームで連結\n",
    "    df = pd.DataFrame()\n",
    "    for l in skf_logits_list:\n",
    "        df = df.append(l)\n",
    "    df.index = skf_val_index_list\n",
    "    df = df.sort_index()\n",
    "    if span == 0:\n",
    "        df.columns = [f'bert_{col}_{i}' for i in range(5)]\n",
    "    if (span == 365) | (span == 730):\n",
    "        df.columns = [f'bert_{span}_{col}_{i}' for i in range(5)]\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_test_features(df, model_list, col):\n",
    "    # テストデータ用の特徴量作成\n",
    "    input_ids, attention_masks = bert_prep(df, col)\n",
    "    # データセット\n",
    "    test_dataset = TensorDataset(input_ids, \n",
    "                                 attention_masks\n",
    "                                )\n",
    "    # データローダー\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        sampler = SequentialSampler(test_dataset),\n",
    "        batch_size = batch_size\n",
    "    )\n",
    "    # 予測\n",
    "    model_logits_list = []\n",
    "    for model in model_list:\n",
    "        model.cuda()\n",
    "        model.eval()\n",
    "        logits_list = []\n",
    "        for batch_data in test_dataloader:\n",
    "            input_ids = batch_data[0].to(device)\n",
    "            input_masks = batch_data[1].to(device)\n",
    "            with torch.no_grad():   \n",
    "                preds = model(input_ids, \n",
    "                              attention_mask=input_masks)\n",
    "            logits = preds.logits.cpu().numpy()\n",
    "            logits_list.append(logits)\n",
    "            \n",
    "        # reshapeできないためデータフレームを使う\n",
    "        df_logits = pd.DataFrame()\n",
    "        for l in logits_list:\n",
    "            df_logits = df_logits.append(pd.DataFrame(l))\n",
    "        df_logits = sp.special.softmax(df_logits, axis=1)\n",
    "        model_logits_list.append(df_logits)\n",
    "        model.cpu()\n",
    "    # モデル予測の平均をとる\n",
    "    last_logits_list = []\n",
    "    for i in range(len(model_logits_list[0])):\n",
    "        last_logits_list.append(np.array(model_logits_list)[:,i,:].mean(axis=0))\n",
    "    # テストデータの特徴量データフレーム\n",
    "    df_test_bert = pd.DataFrame(last_logits_list)\n",
    "    if span == 0:\n",
    "        df_test_bert.columns = [f'bert_test_{col}_{i}' for i in range(5)]\n",
    "    if (span == 365) | (span == 730):\n",
    "        df_test_bert.columns = [f'bert_{span}_test_{col}_{i}' for i in range(5)]\n",
    "    \n",
    "    return df_test_bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc9dcd2-133c-43c7-85cf-6b5f90b0594c",
   "metadata": {},
   "source": [
    "### BERT訓練〜特徴量作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78bdc5ec-8687-4f96-a1d3-ab6eb7ae5b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "1 epoch\n",
      "==========\n",
      "train_loss: 984.5 || val_loss: 232.43 | logloss: 0.9297 | accuracy: 0.58\n",
      "train@\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  2% | 33% |\n",
      "del@\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 20% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "1 epoch\n",
      "==========\n",
      "train_loss: 980.15 || val_loss: 234.32 | logloss: 0.9373 | accuracy: 0.58\n",
      "train@\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 33% |\n",
      "del@\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  1% | 21% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "1 epoch\n",
      "==========\n",
      "train_loss: 979.59 || val_loss: 235.31 | logloss: 0.9413 | accuracy: 0.57\n",
      "train@\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 17% | 33% |\n",
      "del@\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 20% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "1 epoch\n",
      "==========\n",
      "train_loss: 985.69 || val_loss: 232.21 | logloss: 0.9288 | accuracy: 0.57\n",
      "train@\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 29% | 33% |\n",
      "del@\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 22% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "1 epoch\n",
      "==========\n",
      "train_loss: 986.05 || val_loss: 232.07 | logloss: 0.9283 | accuracy: 0.57\n",
      "train@\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 18% | 33% |\n",
      "del@\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  1% | 21% |\n",
      "8522\n",
      "Done!\n",
      "Wall time: 13min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# トークナイザ\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "skf_val_index_list = []\n",
    "skf_model_list = []\n",
    "skf_logits_list = []\n",
    "df_bert = pd.DataFrame()\n",
    "df_test_bert = pd.DataFrame()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# バッチサイズ\n",
    "if (col=='title') | (col=='keyword'):\n",
    "    batch_size = 32\n",
    "if col=='story':\n",
    "    batch_size = 16\n",
    "\n",
    "# 前処理\n",
    "input_ids, attention_masks, labels = bert_prep(df_train, col)\n",
    "\n",
    "for train_index, val_index in skf.split(input_ids, labels):\n",
    "\n",
    "    # 特徴量作成用\n",
    "    skf_val_index_list += val_index.tolist()\n",
    "\n",
    "    # データセット\n",
    "    train_dataset = TensorDataset(input_ids[train_index], \n",
    "                                  attention_masks[train_index], \n",
    "                                  labels[train_index])\n",
    "    val_dataset = TensorDataset(input_ids[val_index], \n",
    "                                attention_masks[val_index], \n",
    "                                labels[val_index])\n",
    "\n",
    "    # データローダー\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        batch_size = batch_size\n",
    "    )\n",
    "    validation_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        sampler = SequentialSampler(val_dataset),\n",
    "        batch_size = batch_size\n",
    "    )\n",
    "\n",
    "    # modelをロード\n",
    "    clf = BertForSequenceClassification\n",
    "    model = clf.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels = 5,\n",
    "        output_attentions = False,\n",
    "        output_hidden_states = False\n",
    "    )\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5) # ▲1e-5\n",
    "\n",
    "    # 訓練\n",
    "    # epoch数\n",
    "    if MODEL_NAME == 'cl-tohoku/bert-base-japanese-whole-word-masking':\n",
    "        max_epoch = 1\n",
    "    if MODEL_NAME == 'cl-tohoku/bert-base-japanese-v2':\n",
    "        if (col=='title') | (col=='keyword'):\n",
    "            max_epoch = 2\n",
    "        if col=='story':\n",
    "            max_epoch = 1\n",
    "    \n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    model_logits_list = []\n",
    "    \n",
    "    model.cuda()\n",
    "\n",
    "    for epoch in range(max_epoch):\n",
    "        print('==========')\n",
    "        print(f'{epoch+1} epoch')\n",
    "        print('==========')\n",
    "        \n",
    "        # 訓練\n",
    "        train_loss = train()\n",
    "        train_loss_list.append(train_loss)\n",
    "        \n",
    "        # 検証\n",
    "        val_loss, logloss, acc, df_logits = validation()\n",
    "        val_loss_list.append(val_loss)\n",
    "        # logits_listは最終epochのみ保存(特徴量化用)\n",
    "        if epoch+1 == max_epoch:\n",
    "            skf_logits_list.append(df_logits)\n",
    "        \n",
    "        print('train_loss:', round(train_loss, 2), end='')\n",
    "        print(' || val_loss:', round(val_loss, 2), \n",
    "              '| logloss:', round(logloss, 4),\n",
    "              '| accuracy:', round(acc, 2))\n",
    "    \n",
    "    model.cpu()\n",
    "    skf_model_list.append(model)\n",
    "    # モデル削除前後のメモリの使用量確認\n",
    "    print('train@')\n",
    "    GPUtil.showUtilization()\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    print('del@')\n",
    "    GPUtil.showUtilization()\n",
    "\n",
    "#     break # テスト用\n",
    "\n",
    "# 訓練データ用の特徴量作成\n",
    "df_bert = pd.concat([df_bert, make_train_features(skf_logits_list, skf_val_index_list, col)], axis=1)\n",
    "\n",
    "# テストデータ用の特徴量作成\n",
    "df_test_bert = pd.concat([df_test_bert, make_test_features(raw_test, skf_model_list, col)], axis=1)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c536a9b0-e2f6-44b9-830a-25ea73678893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの名前指定\n",
    "if MODEL_NAME == 'cl-tohoku/bert-base-japanese-whole-word-masking':\n",
    "    model_name = ''\n",
    "if MODEL_NAME == 'cl-tohoku/bert-base-japanese-v2':\n",
    "    model_name = '_v2_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56b1ff54-d7fe-4458-93aa-e3dd5a1a98d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# カラムにモデル名を入れる\n",
    "df_bert.columns = [(col[:-1] + model_name[1:] + col[-1]) for col in df_bert.columns]\n",
    "df_test_bert.columns = [(col[:-1] + model_name[1:] + col[-1]) for col in df_test_bert.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67f24004-ff10-48d0-84fe-7de8bf0bf843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(       bert_keyword_0  bert_keyword_1  bert_keyword_2  bert_keyword_3  \\\n",
       " 0            0.254550        0.573901        0.141917        0.024596   \n",
       " 1            0.193701        0.486782        0.243959        0.058443   \n",
       " 2            0.236993        0.559316        0.161463        0.034647   \n",
       " 3            0.175567        0.560525        0.232173        0.026142   \n",
       " 4            0.163415        0.535045        0.221831        0.064523   \n",
       " ...               ...             ...             ...             ...   \n",
       " 39995        0.240248        0.546653        0.168535        0.034037   \n",
       " 39996        0.369958        0.471106        0.129718        0.021965   \n",
       " 39997        0.423704        0.468815        0.091772        0.012945   \n",
       " 39998        0.316698        0.452462        0.183636        0.039967   \n",
       " 39999        0.483522        0.360493        0.123212        0.024471   \n",
       " \n",
       "        bert_keyword_4  \n",
       " 0            0.005035  \n",
       " 1            0.017115  \n",
       " 2            0.007581  \n",
       " 3            0.005592  \n",
       " 4            0.015186  \n",
       " ...               ...  \n",
       " 39995        0.010527  \n",
       " 39996        0.007253  \n",
       " 39997        0.002763  \n",
       " 39998        0.007237  \n",
       " 39999        0.008303  \n",
       " \n",
       " [40000 rows x 5 columns],\n",
       "       bert_test_keyword_0  bert_test_keyword_1  bert_test_keyword_2  \\\n",
       " 0                0.400530             0.467150             0.107183   \n",
       " 1                0.014372             0.110278             0.355660   \n",
       " 2                0.802065             0.187028             0.008615   \n",
       " 3                0.443826             0.441210             0.088095   \n",
       " 4                0.773424             0.211950             0.011621   \n",
       " ...                   ...                  ...                  ...   \n",
       " 8517             0.775401             0.210342             0.011748   \n",
       " 8518             0.711539             0.271604             0.014189   \n",
       " 8519             0.848495             0.142930             0.006557   \n",
       " 8520             0.736100             0.247777             0.013044   \n",
       " 8521             0.845998             0.145035             0.006766   \n",
       " \n",
       "       bert_test_keyword_3  bert_test_keyword_4  \n",
       " 0                0.019545             0.005592  \n",
       " 1                0.335390             0.184301  \n",
       " 2                0.001584             0.000708  \n",
       " 3                0.020585             0.006285  \n",
       " 4                0.002133             0.000873  \n",
       " ...                   ...                  ...  \n",
       " 8517             0.001777             0.000732  \n",
       " 8518             0.001987             0.000681  \n",
       " 8519             0.001344             0.000675  \n",
       " 8520             0.002204             0.000875  \n",
       " 8521             0.001446             0.000754  \n",
       " \n",
       " [8522 rows x 5 columns])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bert, df_test_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc8695fb-e7b0-4ecb-968c-ea38d4512048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル名を入れて保存\n",
    "if span == 0:\n",
    "    filename = BASE_DIR + f'df_bert_{col}{model_name[:-1]}.pkl'\n",
    "if (span == 365) | (span == 730):\n",
    "    filename = BASE_DIR + f'df_bert_{span}_{col}{model_name[:-1]}.pkl'\n",
    "with open(filename, mode='wb') as f:\n",
    "    pickle.dump(df_bert, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75418227-7ca5-42a9-b43a-2207c300f5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "if span == 0:\n",
    "    test_filename = BASE_DIR + f'df_test_bert_{col}{model_name[:-1]}.pkl'\n",
    "if (span == 365) | (span == 730):\n",
    "    test_filename = BASE_DIR + f'df_test_bert_{span}_{col}{model_name[:-1]}.pkl'\n",
    "with open(test_filename, mode='wb') as f:\n",
    "    pickle.dump(df_test_bert, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971015c7-bb04-4d3f-9702-0586339efaab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-showcode": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
